---
title: "Lab08_Shrinkage"
author: "Pouria"
date:   "March 21, 2022"
output:  html_document
        
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, warning=F, message=F}
library(magrittr) # You can use %>% for nested functions in data manipulation
# library(dplyr)    # For data manipulation
library(tidyr)    # For tidying your data (columns and rows)
library(ggplot2)  # For plotting
# library(boot)     # For bootstrapping and cross-validation
# library(nlme)     # For mixed-effect models
library(leaps)
library(glmnet)
# Feel free to add your libraries of interest if needed

```


## Lab Prompt (Your version)

**Exercise 8 page 262.**

Questions a, b, c, e.  (in e use 10-fold CV)

Then:

Generate plots like Figure 6.13. (with the vertical line for the optimal lambda). Comment

Now fit a ridge regression model. Find the optimal lambda via CV

Generate a plot like Figure 6.13 for the ridge model. Comment

Compare the 10-fold CV error for the lasso and ridge for the optimal lambda and for the full linear regression model (lambda = 0). What do you conclude?
(here the lasso should be better because the X4 to X10 regressors are only noise)





## Lab Prompt (My version)


8(a, b): Generate the synthetic data

**Book**:
Use the `rnorm()` function to generate a predictor `X` of length
`n = 100`, as well as a noise vector  of length `n = 100`.


**Recommended**:

- Set the seed to `111`.

- Use uniform distribution for generating `X` to range from `-5` to `5`

- Use normal distribution for the residual $\epsilon$.


Generate a response vector `y` according to the model

$$ y = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \epsilon$$ 

- Sample four integers ranging up to `5` with replacement. Use these values for $\beta_0$ through $\beta_3$, respectively.


- Plot `y` as a function of `X`



8(c): Perform Best subset selection

Use the `regsubsets()` to perform best subset selection in order to choose the best model containing the predictors $X$, $X^2$,..., $X^{10}$. What is the best model obtained according to `Cp`, `BIC`, and `adjr2`? 

Further, show some plots to provide evidence for your answer. 

- Scale Plot

- BIC against number of variables

- Report the coefficients of the best model obtained.



8(d): Perform Ridge Regression

Now fit a **ridge** model to the simulated data, again using $X$, $X^2$,..., $X^{10}$ as predictors. Use **10-fold** cross-validation. 

- Select the optimal value of $\lambda_{opt}$. 

- Plot MSE as a function of $\lambda$.

- Generate CV error and standardized coefficients plots against coefficients relative L2-norms. (similar to Figure 6.13 including the dotted vertical line representing those values corresponding to the optimal $\lambda$). Comment.

- Report the resulting coefficient estimates, and discuss the results obtained.



8(e): Perform Lasso Regression

Now fit a **lasso** model to the simulated data, again using $X$, $X^2$,..., $X^{10}$ as predictors. Use **10-fold** cross-validation.

- Select the optimal value of $\lambda_{opt}$. 

- Plot MSE as a function of $\lambda$. 

- Generate CV error and standardized coefficients plots against coefficients relative L1-norms. (similar to Figure 6.13 including the dotted vertical line representing those values corresponding to the optimal $\lambda$). Comment.

- Report the resulting coefficient estimates, and discuss the results obtained.



8(f): Compare Lasso and Ridge

Compare the 10-fold CV error and the coefficients obtained from the lasso and ridge for:

(1) their optimal $\lambda$ 

(2) their full linear regression model ($\lambda = 0$). 

What do you conclude?







================================================================================
________________________________________________________________________________

# Answers


## 8(a, b): Generate the synthetic data

**Book**:
Use the `rnorm()` function to generate a predictor `X` of length
`n = 100`, as well as a noise vector  of length `n = 100`.


**Recommended**:

- Set the seed to `111`.

- Use uniform distribution for generating `X` to range from `-5` to `5`

- Use normal distribution for the residual $\epsilon$.


Generate a response vector `y` according to the model

$$ y = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \epsilon$$ 

- Sample four integers ranging up to `5` with replacement. Use these values for $\beta_0$ through $\beta_3$, respectively.


```{r}

set.seed(111)

# X = rnorm(100, 0, 5)
x = runif(100, -5, 5)

eps = rnorm(100, 0, 20)

beta = sample(5, 11, replace = T)

beta[5:11] = 0


b0 = beta[1]
b1 = beta[2]
b2 = beta[3]
b3 = beta[4]

# y = b0 + b1*x + b2*x^2 + b3*x^3 + eps



# Matrix: XW
X = matrix(rep(1, 10*100), nrow = 100)

for (i in 1:10){
  X[,i] <- x^i
}

y = beta[1] + X%*%beta[2:11] + eps




```


## 8(a, b): plot `y` as a function of `X`

```{r}
datapoints = data.frame(X, y)
ggplot(datapoints) +
  geom_point(mapping = aes(X1, y), col = "black") +
  theme_bw() +
  xlab("X")

```





________________________________________________________________________________

## 8(c): Perform Best subset selection

Use the `regsubsets()` to perform best subset selection in order to choose the best model containing the predictors $X$, $X^2$,..., $X^{10}$. What is the best model obtained according to `Cp`, `BIC`, and `adjr2`? 

```{r}
regfit <- regsubsets(y~., datapoints, nvmax = 10)
regfit.summary <- summary(regfit)
regfit.summary
```



```{r}
adjr.sub<- regfit.summary$adjr2
Cp.sub  <- regfit.summary$cp
BIC.sub <- regfit.summary$bic
```

## Create plots
Show some plots to provide evidence for your answer. 

- Scale Plot

```{r}

plot(regfit ,scale="bic")

```

## Create plots
Show some plots to provide evidence for your answer. 

- BIC against number of variables

```{r}
# df.bestsubset <- data.frame(adjr.sub, Cp.sub, BIC.sub)
id.min_bic <- which.min(BIC.sub)
min_bic <- min(BIC.sub)

ggplot() +
  geom_point(aes(1:dim(X)[2], BIC.sub), col = "black", size = 3) + 
  geom_line(aes(1:dim(X)[2], BIC.sub), col = "black") +
  geom_point(aes(id.min_bic, min_bic), col = "red", size = 6) +
  xlab("Number of Variables in Best Subsets Selected") +
  ylab("BIC") +
  theme_bw()

```


## Coefficients from best subset
- Report the coefficients of the best model obtained.

```{r}
coef(regfit, which.min(regfit.summary$bic))
```






________________________________________________________________________________

## 8(d): Perform Ridge Regression

Now fit a **ridge** model to the simulated data, again using $X$, $X^2$,..., $X^{10}$ as predictors. Use **10-fold** cross-validation. 

- Select the optimal value of $\lambda_{opt}$. 

- Create plots of the cross-validation error as a function of $\lambda$.

```{r}
cv.out.ridge <- cv.glmnet(X, y, alpha=0)
plot(cv.out.ridge)
# log(cv.out$lambda.min)
# log(cv.out$lambda.1se)
# 
# cv.out$cvm[cv.out$lambda==cv.out$lambda.1se] - cv.out$cvm[which.min(cv.out$lambda)]
# 
# cv.out$cvsd[which.min(cv.out$lambda)]
# 
# df.cv <- data.frame(cv.m = cv.out$cvm, cv.sd = cv.out$cvsd, lambda = cv.out$lambda)
# 
# ggplot(df.cv) +
#   geom_point(aes(lambda, cv.m))
```

```{r}
lambda.opt.ridge <- cv.out.ridge$lambda.1se
lambda.opt.ridge
```


## 8(d): Ridge: Other plots

- Generate CV error and standardized coefficients plots against coefficients relative L2-norms. (similar to Figure 6.13 including the dotted vertical line representing those values corresponding to the optimal $\lambda$). Comment.

```{r}
CV= cv.out.ridge$cvm
lam = cv.out.ridge$lambda

out.ridge <- glmnet(X, y, alpha=0)

L1.norm.full <- sum(abs(coef(lm(y~X))))


Coeff.Mat = matrix(rep(0, length(lam)*10), nrow = length(lam))
L1.norm = 0
for (i in 1:length(lam)){
  co = as.array(predict (out.ridge , type = "coefficients", s = lam[i]))
  Coeff.Mat[i,] = co[2:11]
  L1.norm[i] = sum(abs(Coeff.Mat[i,]))/L1.norm.full
}

L1.norm.opt <- L1.norm[lam==lambda.opt.ridge]
CV.ridge.opt <- CV[lam==lambda.opt.ridge]


df <- data.frame(lam, Coeff.Mat, L1.norm, CV) %>%
  pivot_longer(cols = 2:11, names_to = "Variable", values_to = "Value")

ggplot(df) +
  geom_line(aes(L1.norm, CV), col="red", size = 1.2) +
  geom_vline(aes(xintercept = L1.norm.opt), linetype="dotted", size = 1.1) +
  theme_bw() +
  xlab("Relative L1 Norm") +
  ylab("Cross-Validation Error")

ggplot(df) +
  geom_line(aes(L1.norm, Value, col=Variable), size = 1.2) +
  geom_vline(aes(xintercept = L1.norm.opt), linetype="dotted", size = 1.1) +
  theme_bw() +
  xlab("Relative L1 Norm") +
  ylab("Standardized Coefficients")

```


## 8(d): Ridge: Coefficients
- Report the resulting coefficient estimates, and discuss the results obtained.

```{r}
Coeff.ridge <- predict (out.ridge , type = "coefficients", s = lambda.opt.ridge)
```





________________________________________________________________________________

## 8(e): Perform Lasso Regression

Now fit a **lasso** model to the simulated data, again using $X$, $X^2$,..., $X^{10}$ as predictors. Use **10-fold** cross-validation.

- Select the optimal value of $\lambda_{opt}$. 

- Create plots of the cross-validation error as a function of $\lambda$. 

```{r}
cv.out.lasso <- cv.glmnet(X, y, alpha=1)
plot(cv.out.lasso)
```

```{r}
lambda.opt.lasso <- cv.out.lasso$lambda.1se 
lambda.opt.lasso
```


## 8(e): Lasso: Other plots

- Generate CV error and standardized coefficients plots against coefficients relative L1-norms. (similar to Figure 6.13 including the dotted vertical line representing those values corresponding to the optimal $\lambda$). Comment.

```{r}
CV = cv.out.lasso$cvm
lam = cv.out.lasso$lambda

out.lasso <- glmnet(X, y, alpha=1)

L1.norm.full <- sum(abs(coef(lm(y~X))))


Coeff.Mat = matrix(rep(0, length(lam)*10), nrow = length(lam))
L1.norm = 0
for (i in 1:length(lam)){
  co = as.array(predict (out.lasso , type = "coefficients", s = lam[i]))
  Coeff.Mat[i,] = co[2:11]
  L1.norm[i] = sum(abs(Coeff.Mat[i,]))/L1.norm.full
}

L1.norm.opt <- L1.norm[lam==lambda.opt.lasso]
CV.lasso.opt <- CV[lam==lambda.opt.lasso]


df <- data.frame(lam, Coeff.Mat, L1.norm, CV) %>%
  pivot_longer(cols = 2:11, names_to = "Variable", values_to = "Value")

ggplot(df) +
  geom_line(aes(L1.norm, CV), col="red", size = 1.2) +
  geom_vline(aes(xintercept = L1.norm.opt), linetype="dotted", size = 1.1) +
  theme_bw() +
  xlab("Relative L1 Norm") +
  ylab("Cross-Validation Error")

ggplot(df) +
  geom_line(aes(L1.norm, Value, col=Variable), size = 1.2) +
  geom_vline(aes(xintercept = L1.norm.opt), linetype="dotted", size = 1.1) +
  theme_bw() +
  xlab("Relative L1 Norm") +
  ylab("Standardized Coefficients")

```



## 8(e): Lasso: Coefficients
- Report the resulting coefficient estimates, and discuss the results obtained.

```{r}
Coeff.lasso <- predict (out.lasso , type = "coefficients", s = lambda.opt.lasso)
```




________________________________________________________________________________

## 8(f): Compare Lasso and Ridge

Compare the 10-fold CV error and the coefficients obtained from the lasso and ridge for:

(1) their optimal $\lambda$ 

(2) their full linear regression model ($\lambda = 0$). 

What do you conclude?


```{r}
data.frame(CV.lasso.opt, CV.ridge.opt)
```

```{r}
data.frame(as.array(Coeff.ridge), as.array(Coeff.lasso),  beta) %>%
  set_colnames(c("Ridge", "Lasso",  "True"))
```


