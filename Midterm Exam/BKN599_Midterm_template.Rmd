--- 
title:  "Midterm Exam - BKN 599"
author: "FirstName LastName"
date:   "March 7, 2022"
output:  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## Description of the dataset: 

This dataset includes the lifetime earnings (or cumulative earnings) (`Earning`) of a sample of age-matched individuals as a function of years of working (`Year`). The individuals either have PhD or they don't noted in variable `Degree`, so the baseline year begins with when the PhD individuals have graduated; thus looking at data it makes sense that those with no PhD could start with higher number because they have already been in the job market while those with PhD were studying. The dataset have been sampled from 5 different groups (`Group`) within each `PhD` and `Not-PhD` degrees. We aim to take a closer look into this dataset and interpret it using our knowledge of exploratory analysis and statistical learning/modeling that we have gained so far.


## Problem 1: 
Implement a **simple linear regression** analysis (call the model `fit1`) to predict the lifetime earning (`Earning`) as the response variable with `Year`, `Degree` and their interactions as the predictors.

1A. **Summarize** the model and report your findings on what predictors have **significant effects** on variations in `Earning`. If there are more than one model, report all their **intercepts and slopes**.

1B. **Plot the fits**. Ensure to include the **data points** split into "PhD" and "not-PhD" degrees. Read these results and report your **inference** in general terms based on the variable names.

1C. Plot the **diagnostics**. Check all the linear model **assumptions** and make comments.


1D. Use both SE equation (provided below or Eq. 8 in the book) and bootstrapping (200 times) methods to calculate the standard error on the coefficient corresponding to the variable `Year` and compare them. Which one is a more accurate estimate of standard error and why?

$$SE(\hat \beta_{Year})^2=\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2}$$







## Problem 2: 

Now fit a new linear model (call it `fit2`) by adding a square term on `Year` and its interaction with `Degree` to the relationship of your model `fit1` to predict `Earning` again.

2A. **Summarize** the model and report your findings on **significant effects**. If there are more than one model, report all their **intercepts and slopes**.

2B. **Plot the fits**. Ensure to include the **data points** split into "PhD" and "not-PhD" degrees. Read these results and report your **inference** in general terms based on the variable names.

2C. Use `ANOVA` to compare the model with quadratic term and the simple model in Problem 1. Comment on significance of the difference.

2D. Now, of these the two models determine which one is better using a 10-fold cross-validation.




## Problem 3: 

In this problem, we aim to fit a **linear mixed effect model** to the data to predict `Earning` by other continuous and categorical variables in the dataset.


3A. First, plot the residuals of `fit1` (no square term) in box plot against the groups ("P1",...,"P5", "N1",..."N5") with "PhD" and "not-PhD" denoted in different colors again. Is `fit1` sufficient to explain the variations in `Earnings`? Explain why or why not? What effects have not been accounted for?

3B. **Fit a linear mixed effect model** with a baseline similar to `fit1` (`Year`, `Degree` and interactions) and a random effect of `Group`. 

- Perform this with both a general and a diagonal covariance matrix structure and save the best fit to `lm.fit.mixed`. (show your work on how you chose the better mixed-effect fit)

3C. **Fixed, random, and mixed effects**; Tease out the fixed-effect coefficients and the random-effect coefficients (for different groups); Only keep the significant ones. 

3D. Plot the fixed-effect fits and mixed-effect fits per group on one graph.

3E. Plot the residuals of your mixed-effect model and compare it to 3A. How do you explain any similarity or difference between them?




## Problem 4: 

**Bonus (partially or fully)**

Take a linear regression of the entire data for one linear fit in predicting `Earning` by `Year` (no `Degree` and no `Group` effects). 

4A. Take the intercept and slope coefficients obtained from this fit and then systematically evaluate the residual sum of squares (RSS, Eq. 3.3) around this optimal set of coefficients(range in `-/+ 10,000%` of the coefficients from the `lm()` with step size of `100` in your for-loop) to systematically create a contour plot of `RSS` against pairs of $\beta_0$ and $\beta_1$ similar to Fig. 3.2 in the book. 

4B. Repeat above this time for `Year` centered about its mean value (You should first do a new fit and find its intercept and slopes with `lm()`)

4C. Compare the shapes of contour plots in these two centered and un-centered cases. What is the phenomenon you observe? Comment on your results.




==================================================================================



# Libraries

```{r libraries, warning=F, message=F}
library(magrittr) # You can use %>% for nested functions in data manipulation
library(dplyr)    # For data manipulation
library(tidyr)    # For tidying your data (columns and rows)
library(ggplot2)  # For plotting
library(boot)     # For bootstrapping and cross-validation
library(nlme)     # For mixed-effect models


# Feel free to add your libraries of interest if needed

```





# Answers



## Problem 1

Implement a **simple linear regression** analysis (call the model `fit1`) to predict the lifetime earning (`Earning`) as the response variable with `Year`, `Degree` and their interactions as the predictors.

**1A.** **Summarize** the model and report your findings on what predictors have **significant effects** on variations in `Earning`. If there are more than one model, report all their **intercepts and slopes**.


```{r}

```


**1B.** **Plot the fits**. Ensure to include the **data points** split into "PhD" and "Not-PhD" degrees. Read these results and report your **interpretations** of them.

```{r}

```


**1C.** Plot the **diagnostics**. Check all the linear model **assumptions** and make comments.

```{r}

```





**1D.** Use both SE equation (provided below or Eq. 8 in the book) and bootstrapping (200 times) methods to calculate the standard error on the coefficient corresponding to the variable `Year` and compare them. Which one is a more accurate estimate of standard error and why?

$$SE(\hat \beta_{Year})^2=\frac{\sigma^2}{\sum_{i=1}^n(x_i-\bar{x})^2}$$



```{r}

```






## Problem 2

Now fit a new linear model (call it `fit2`) by adding a square term on `Year` and its interaction with `Degree` to the relationship of your model `fit1` to predict `Earning` again.

**2A.** **Summarize** the model and report your findings on **significant effects**. If there are more than one model, report all their **intercepts and slopes**.**


```{r}

```




**2B.** **Plot the fits**. Ensure to include the **data points** split into "PhD" and "Not-PhD" degrees. Read these results and report your **inference** in general terms based on the variable names.



```{r}

```




**2C.** Use `ANOVA` to compare the model with quadratic term and the simple model in Problem 1. Comment on significance of the difference.



```{r}

```






**2D.** Now, of these the two models determine which one is better using a 10-fold cross-validation.


```{r}

```




## Problem 3
In this problem, we aim to fit a linear mixed effect model to the data to predict `Earning` by other continuous and categorical variables in the dataset.

**3A.** First, plot the residuals of `fit1` (no square term) in box plot against the groups ("P1",...,"P10", "N1",..."N10") with "PhD" and "not-PhD" denoted in different colors again. Is `fit1` sufficient to explain the variations in `Earnings`? Explain why or why not? What effects have not been accounted for?


```{r}

```






**3B.** **Fit a linear mixed effect model** with a baseline similar to `fit1` (`Year`, `Degree` and interactions) and a random effect of `Group`.

- Perform this with both a general and a diagonal covariance matrix structure and save the best fit to `lm.fit.mixed`. (show your work on how you chose the better mixed-effect fit)


```{r}

```


```{r}

```





**3C.** **Fixed, random, and mixed effects**; Tease out the fixed-effect coefficients and the random-effect coefficients (for different groups); Only keep the significant ones.


```{r}

```





**3D.** Plot the fixed-effect fits and mixed-effect fits per group on one graph. Are there any groups for which the fixed effect would explain the trend in the data?


```{r}

```


```{r}

```



**3E.** Plot the residuals of your mixed-effect model and compare it to 3A. Is it different from 3A? Why or why not?


```{r}

```

```{r}

```







## Problem 4

**4A.** Take a linear regression of the entire data for one linear fit in predicting `Earning` by `Year` (no `Degree` and no `Group` effects). Take the intercept and slope coefficients obtained from this fit and then systematically evaluate the residual sum of squares (RSS, Eq. 3.3) around this optimal set of coefficients(-/+10000% with 100 of step size in your for-loop) to systematically create contour plot similar to Fig. 3.2 in the book.


```{r}

```



**4B.** Repeat this for un-centered data. (You should do the fit for un-centered data)

```{r}

```


**4C.** Compare the shapes of contour plots in these two centered and un-centered cases. What is the phenomenon you observe? Comment on your results.

```{r}


```


